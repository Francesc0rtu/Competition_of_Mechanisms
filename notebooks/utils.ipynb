{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import json\n",
    "from src.model import WrapHookedTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.utils import get_act_name\n",
    "from functools import partial\n",
    "from transformer_lens import patching\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "dataset = json.load(open(\"P1GL.json\"))\n",
    "class Dataset:\n",
    "    def __init__(self, dataset, model:WrapHookedTransformer):\n",
    "        self.dataset = [\n",
    "            {\"prompt\": original_dict[\"pair\"][i],\n",
    "             \"target\": original_dict[\"answer\"][i],\n",
    "             \"length\": original_dict[\"length\"]}\n",
    "        for original_dict in dataset\n",
    "        for i in range(2)\n",
    "        ]\n",
    "        self.add_premise(model)\n",
    "        self.dataset_per_length = self.split_for_lenght()\n",
    "\n",
    "    def add_premise(self, model:WrapHookedTransformer):\n",
    "        for d in self.dataset:\n",
    "            ortoghonal_token = model.to_orthogonal_tokens(d[\"target\"])\n",
    "            premise = d[\"prompt\"] + ortoghonal_token + \" \" + d[\"prompt\"]\n",
    "            d[\"premise_prompt\"] = premise\n",
    "            d[\"orthogonal_token\"] = ortoghonal_token\n",
    "            d[\"length\"] = len(model.to_str_tokens(premise))\n",
    "        return self\n",
    "\n",
    "    def split_for_lenght(self):\n",
    "        dataset_per_length = {}\n",
    "        for d in self.dataset:\n",
    "            length = d[\"length\"]\n",
    "            if length not in dataset_per_length:\n",
    "                dataset_per_length[length] = []\n",
    "            dataset_per_length[length].append(d)\n",
    "        return dataset_per_length\n",
    "    \n",
    "    def logits(self, model:WrapHookedTransformer):\n",
    "        logits_per_length = {}\n",
    "        for length, dataset in self.dataset_per_length.items():\n",
    "            input_ids = model.to_tokens([d[\"premise_prompt\"] for d in dataset])\n",
    "            logits_per_length[length] = model(input_ids)\n",
    "        return logits_per_length\n",
    "  \n",
    "    def get_tensor_token(self,model):\n",
    "        tensor_token_per_length = {}\n",
    "        for length, dataset in self.dataset_per_length.items():\n",
    "            if length not in tensor_token_per_length:\n",
    "                tensor_token_per_length[length] = {}\n",
    "            tensor_token_per_length[length][\"target\"] = model.to_tokens([d[\"target\"] for d in dataset], prepend_bos=False)\n",
    "            tensor_token_per_length[length][\"orthogonal_token\"] = model.to_tokens([d[\"orthogonal_token\"] for d in dataset], prepend_bos=False)\n",
    "        \n",
    "        for length, tensor in tensor_token_per_length.items():\n",
    "            tensor_token_per_length[length][\"target\"] = tensor_token_per_length[length][\"target\"].squeeze(1)\n",
    "            tensor_token_per_length[length][\"orthogonal_token\"] = tensor_token_per_length[length][\"orthogonal_token\"].squeeze(1)\n",
    "        return tensor_token_per_length\n",
    "    \n",
    "model = WrapHookedTransformer.from_pretrained(\"gpt2\", device=\"cpu\")\n",
    "dataset = Dataset(dataset, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_length = dataset.logits(model)\n",
    "tokens_dict_per_length = dataset.get_tensor_token(model)\n",
    "\n",
    "delta_per_length = {}\n",
    "for max_len in list(tokens_dict_per_length.keys()):\n",
    "    if max_len not in delta_per_length:\n",
    "        delta_per_length[max_len] = {}\n",
    "    # probs = torch.softmax(logits_per_length[max_len], dim=-1)\n",
    "    probs = logits_per_length[max_len]\n",
    "    batch_index = torch.arange(probs.shape[0])\n",
    "    delta_per_length[max_len] = probs[batch_index,-1, tokens_dict_per_length[max_len][\"target\"]] - probs[batch_index,-1, tokens_dict_per_length[max_len][\"orthogonal_token\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0146)\n",
      "tensor(0.3278)\n"
     ]
    }
   ],
   "source": [
    "# get the mean of the logits of target and orthogonal token\n",
    "target_logits_sum = 0\n",
    "orthogonal_logits_sum = 0\n",
    "for max_len in list(logits_per_length.keys()):\n",
    "    probs = logits_per_length[max_len]\n",
    "    probs = torch.softmax(logits_per_length[max_len], dim=-1)\n",
    "    batch_index = torch.arange(probs.shape[0])\n",
    "    target_logits_sum += probs[batch_index,-1, tokens_dict_per_length[max_len][\"target\"]].mean()\n",
    "    orthogonal_logits_sum += probs[batch_index,-1, tokens_dict_per_length[max_len][\"orthogonal_token\"]].mean()\n",
    "\n",
    "target_logits_sum /= len(list(logits_per_length.keys()))\n",
    "orthogonal_logits_sum /= len(list(logits_per_length.keys()))\n",
    "print(target_logits_sum)\n",
    "print(orthogonal_logits_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 50257])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(d[\"premise_prompt\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who win?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_winners = []\n",
    "orthogonal_winners = []\n",
    "for d in dataset.dataset:\n",
    "    prob = torch.softmax(model(d[\"premise_prompt\"]), dim=-1)\n",
    "    pred = prob[:,-1,:].argmax(-1)\n",
    "    if model.to_string(pred) == d[\"target\"]:\n",
    "        factual_winners.append(d)\n",
    "    if model.to_string(pred) == d[\"orthogonal_token\"]:\n",
    "        orthogonal_winners.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.5310344827586206\n"
     ]
    }
   ],
   "source": [
    "print(len(factual_winners)/len(dataset.dataset))\n",
    "print(len(orthogonal_winners)/len(dataset.dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
