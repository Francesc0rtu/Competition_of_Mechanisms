{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fortu/miniconda3/envs/mpi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import json\n",
    "from src.model import WrapHookedTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.utils import get_act_name\n",
    "from functools import partial\n",
    "from transformer_lens import patching\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"known_1000.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = WrapHookedTransformer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [00:00<00:00, 1903496.07it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for d in tqdm(data, total=len(data)):\n",
    "    dataset.append(\n",
    "        {\"prompt\": d[\"prompt\"],\n",
    "         \"target\": \" \" + d[\"attribute\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1209/1209 [00:38<00:00, 31.70it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_per_length = {}\n",
    "for d in tqdm(dataset, total=len(dataset)):\n",
    "    orthogonal_token = model.to_orthogonal_tokens(d[\"target\"])\n",
    "    d[\"premise\"] = d[\"prompt\"] + orthogonal_token + \" \" + d[\"prompt\"]\n",
    "    d[\"orthogonal_token\"] = orthogonal_token\n",
    "    d[\"length\"] = len(model.to_str_tokens(d[\"premise\"]))\n",
    "    if d[\"length\"] not in dataset_per_length:\n",
    "        dataset_per_length[d[\"length\"]] = []\n",
    "    dataset_per_length[d[\"length\"]].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pytorch dataloader for each length\n",
    "dataloaders = {}\n",
    "for length in sorted(dataset_per_length.keys()):\n",
    "    dataloaders[length] = torch.utils.data.DataLoader(dataset_per_length[length], batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n"
     ]
    }
   ],
   "source": [
    "target_probs_mean = {}\n",
    "orthogonal_probs_mean = {}\n",
    "target_win = {}\n",
    "orthogonal_win = {}\n",
    "other_win = {}\n",
    "target_win_over_orthogonal = {}\n",
    "target_win_over_orthogonal_dataset = []\n",
    "\n",
    "\n",
    "for length in sorted(dataset_per_length.keys()):\n",
    "    # get logits for each example\n",
    "    target_probs_mean[length] = []\n",
    "    orthogonal_probs_mean[length] = []\n",
    "    target_win[length] = 0\n",
    "    orthogonal_win[length] = 0\n",
    "    other_win[length] = 0\n",
    "    target_win_over_orthogonal[length] = 0\n",
    "    for batch in tqdm(dataloaders[length]):\n",
    "        logit = model(batch[\"premise\"])\n",
    "        probs = torch.softmax(logit, dim=-1)\n",
    "        batch_index = torch.arange(probs.shape[0])\n",
    "        target_probs = probs[batch_index, -1, model.to_tokens(batch[\"target\"], prepend_bos=False).squeeze(-1)]\n",
    "        orthogonal_probs = probs[batch_index, -1, model.to_tokens(batch[\"orthogonal_token\"], prepend_bos=False).squeeze(-1)]\n",
    "        predictions = probs[:,-1,:].max(dim=-1)[0]\n",
    "        # for each element of the batch check the prediction and update the win counter\n",
    "        for i in range(len(batch[\"premise\"])):\n",
    "            if target_probs[i] == predictions[i]:\n",
    "                target_win[length] += 1\n",
    "            elif orthogonal_probs[i] == predictions[i]:\n",
    "                orthogonal_win[length] += 1\n",
    "            if target_probs[i] > orthogonal_probs[i]:\n",
    "                target_win_over_orthogonal[length] += 1\n",
    "                target_win_over_orthogonal_dataset.append(batch[i])\n",
    "        \n",
    "        target_probs_mean[length].append(target_probs.mean().item())\n",
    "        orthogonal_probs_mean[length].append(orthogonal_probs.mean().item())\n",
    "    \n",
    "    # mean of logits for each length\n",
    "    target_probs_mean[length] = sum(target_probs_mean[length]) / len(target_probs_mean[length])\n",
    "    orthogonal_probs_mean[length] = sum(orthogonal_probs_mean[length]) / len(orthogonal_probs_mean[length])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target win 0.04052936311000827\n",
      "orthogonal win 0.7014061207609594\n",
      "other win 0.0\n"
     ]
    }
   ],
   "source": [
    "#sum target win and orthogonal win and target_win_over_orthogonal for each length\n",
    "target_win = sum(target_win.values())\n",
    "orthogonal_win = sum(orthogonal_win.values())\n",
    "\n",
    "#print percentages over the total number of examples\n",
    "print(\"target win\", target_win / len(dataset))\n",
    "print(\"orthogonal win\", orthogonal_win / len(dataset))\n",
    "target_win_over_orthogonal = sum(target_win_over_orthogonal.values())\n",
    "print(\"target win over orthogonal\", target_win_over_orthogonal / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target win over orthogonal 0.1728701406120761\n"
     ]
    }
   ],
   "source": [
    "target_win_over_orthogonal = sum(target_win_over_orthogonal.values())\n",
    "print(\"target win over orthogonal\", target_win_over_orthogonal / len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"../data/counterfact.json\"))\n",
    "model = WrapHookedTransformer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21919/21919 [00:00<00:00, 206264.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for d in tqdm(data, total=len(data)):\n",
    "    for i in range(len(d[\"attribute_prompts\"])):\n",
    "        dataset.append(\n",
    "            {\"prompt\": d[\"attribute_prompts\"][i],\n",
    "             \"target\": \" \" + d[\"requested_rewrite\"][\"target_new\"][\"str\"]}\n",
    "        )\n",
    "    \n",
    "    for i in range(len(d[\"neighborhood_prompts\"])):\n",
    "        dataset.append(\n",
    "            {\"prompt\": d[\"neighborhood_prompts\"][i],\n",
    "             \"target\": \" \" + d[\"requested_rewrite\"][\"target_true\"][\"str\"]}\n",
    "        )\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random shuffle\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "dataset = dataset[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438380/438380 [3:47:59<00:00, 32.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "dataset_per_length = {}\n",
    "for d in tqdm(dataset, total=len(dataset)):\n",
    "    orthogonal_token = model.to_orthogonal_tokens(d[\"target\"])\n",
    "    d[\"premise\"] = d[\"prompt\"] + orthogonal_token + \" \" + d[\"prompt\"]\n",
    "    d[\"orthogonal_token\"] = orthogonal_token\n",
    "    d[\"length\"] = len(model.to_str_tokens(d[\"premise\"]))\n",
    "    if d[\"length\"] not in dataset_per_length:\n",
    "        dataset_per_length[d[\"length\"]] = []\n",
    "    dataset_per_length[d[\"length\"]].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pytorch dataloader for each length\n",
    "dataloaders = {}\n",
    "for length in sorted(dataset_per_length.keys()):\n",
    "    dataloaders[length] = torch.utils.data.DataLoader(dataset_per_length[length], batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.67it/s]\n",
      "100%|██████████| 30/30 [00:20<00:00,  1.44it/s]\n",
      "100%|██████████| 27/27 [00:21<00:00,  1.25it/s]\n",
      "100%|██████████| 145/145 [02:25<00:00,  1.00s/it]\n",
      "100%|██████████| 83/83 [01:33<00:00,  1.13s/it]\n",
      " 85%|████████▍ | 257/304 [32:06<05:52,  7.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m target_win_over_orthogonal[length] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataloaders[length]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     logit \u001b[39m=\u001b[39m model(batch[\u001b[39m\"\u001b[39;49m\u001b[39mpremise\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(logit, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     batch_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(probs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:494\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, left_attention_mask, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mnormalization_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_final(residual)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39mif\u001b[39;00m return_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/transformer_lens/components.py:276\u001b[0m, in \u001b[0;36mLayerNormPre.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# [batch, pos, length]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m scale: Union[\n\u001b[1;32m    273\u001b[0m     Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos 1\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    274\u001b[0m     Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos head_index 1\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    275\u001b[0m ] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_scale((x\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\u001b[39m.\u001b[39msqrt())\n\u001b[0;32m--> 276\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_normalized(x \u001b[39m/\u001b[39;49m scale)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_probs_mean = {}\n",
    "orthogonal_probs_mean = {}\n",
    "target_win = {}\n",
    "orthogonal_win = {}\n",
    "other_win = {}\n",
    "target_win_over_orthogonal = {}\n",
    "target_win_dataset = []\n",
    "orthogonal_win_dataset = []\n",
    "\n",
    "\n",
    "for length in sorted(dataset_per_length.keys()):\n",
    "    # get logits for each example\n",
    "    target_probs_mean[length] = []\n",
    "    orthogonal_probs_mean[length] = []\n",
    "    target_win[length] = 0\n",
    "    orthogonal_win[length] = 0\n",
    "    other_win[length] = 0\n",
    "    target_win_over_orthogonal[length] = 0\n",
    "    for batch in tqdm(dataloaders[length]):\n",
    "        logit = model(batch[\"premise\"])\n",
    "        probs = torch.softmax(logit, dim=-1)\n",
    "        batch_index = torch.arange(probs.shape[0])\n",
    "        target_probs = probs[batch_index, -1, model.to_tokens(batch[\"target\"], prepend_bos=False).squeeze(-1)]\n",
    "        if len(model.to_tokens(batch[\"orthogonal_token\"], prepend_bos=False).squeeze(-1).shape) == 2:\n",
    "            orthogonal_tokens = model.to_tokens(batch[\"orthogonal_token\"], prepend_bos=False)[:,0].squeeze(-1)\n",
    "        else:\n",
    "            orthogonal_tokens = model.to_tokens(batch[\"orthogonal_token\"], prepend_bos=False).squeeze(-1)\n",
    "        orthogonal_probs = probs[batch_index, -1, orthogonal_tokens]\n",
    "        predictions = probs[:,-1,:].max(dim=-1)[0]\n",
    "        # for each element of the batch check the prediction and update the win counter\n",
    "        for i in range(len(batch[\"premise\"])):\n",
    "            if target_probs[i] == predictions[i]:\n",
    "                target_win[length] += 1\n",
    "                target_win_dataset.append(\n",
    "                    {\n",
    "                        \"prompt\": batch[\"prompt\"][i],\n",
    "                        \"target\": batch[\"target\"][i],\n",
    "                        \"premise\": batch[\"premise\"][i],\n",
    "                        \"orthogonal_token\": batch[\"orthogonal_token\"][i],\n",
    "                        \"length\": float(batch[\"length\"][i].cpu().detach().numpy().item()),\n",
    "                        \"target_probs\": float(target_probs[i].cpu().detach().numpy().item()),\n",
    "                        \"orthogonal_probs\": float(orthogonal_probs[i].cpu().detach().numpy().item()),\n",
    "                        \n",
    "                    }\n",
    "                \n",
    "                )\n",
    "            elif orthogonal_probs[i] == predictions[i]:\n",
    "                orthogonal_win[length] += 1\n",
    "                orthogonal_win_dataset.append(\n",
    "                    {\n",
    "                        \"prompt\": batch[\"prompt\"][i],\n",
    "                        \"target\": batch[\"target\"][i],\n",
    "                        \"premise\": batch[\"premise\"][i],\n",
    "                        \"orthogonal_token\": batch[\"orthogonal_token\"][i],\n",
    "                        \"length\": float(batch[\"length\"][i].cpu().detach().numpy().item()),\n",
    "                        \"target_probs\": float(target_probs[i].cpu().detach().numpy().item()),\n",
    "                        \"orthogonal_probs\": float(orthogonal_probs[i].cpu().detach().numpy().item()),\n",
    "                        \n",
    "                    }\n",
    "                \n",
    "                )\n",
    "            if target_probs[i] > orthogonal_probs[i]:\n",
    "                target_win_over_orthogonal[length] += 1\n",
    "                # target_win_over_orthogonal_dataset.append(batch[i])\n",
    "        \n",
    "        target_probs_mean[length].append(target_probs.mean().item())\n",
    "        orthogonal_probs_mean[length].append(orthogonal_probs.mean().item())\n",
    "    \n",
    "    # mean of logits for each length\n",
    "    target_probs_mean[length] = sum(target_probs_mean[length]) / len(target_probs_mean[length])\n",
    "    orthogonal_probs_mean[length] = sum(orthogonal_probs_mean[length]) / len(orthogonal_probs_mean[length])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.63s/it]\n",
      "  0%|          | 0/2 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X36sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m target_win_over_orthogonal[length] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataloaders[length]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     logit \u001b[39m=\u001b[39m model(batch[\u001b[39m\"\u001b[39;49m\u001b[39mpremise\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X36sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(logit, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fortu/Competition_of_Mechanisms/notebooks/dataset.ipynb#X36sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     batch_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(probs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:480\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, left_attention_mask, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    477\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    478\u001b[0m         )\n\u001b[0;32m--> 480\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    481\u001b[0m         residual,\n\u001b[1;32m    482\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    483\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    484\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    485\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    486\u001b[0m         left_attention_mask\u001b[39m=\u001b[39;49mleft_attention_mask,\n\u001b[1;32m    487\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/transformer_lens/components.py:1048\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, left_attention_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m   1039\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[39m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\n\u001b[1;32m   1044\u001b[0m         query_input\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(query_input)\n\u001b[1;32m   1045\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39m0.0\u001b[39m \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m   1046\u001b[0m         key_input\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(key_input)\n\u001b[1;32m   1047\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39m0.0\u001b[39m \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m shortformer_pos_embed),\n\u001b[0;32m-> 1048\u001b[0m         value_input\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(value_input),\n\u001b[1;32m   1049\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39mpast_kv_cache_entry,\n\u001b[1;32m   1050\u001b[0m         left_attention_mask\u001b[39m=\u001b[39mleft_attention_mask,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1054\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1055\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m   1056\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mpi/lib/python3.11/site-packages/transformer_lens/components.py:275\u001b[0m, in \u001b[0;36mLayerNormPre.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# [batch, pos, length]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m scale: Union[\n\u001b[1;32m    273\u001b[0m     Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos 1\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    274\u001b[0m     Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos head_index 1\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m--> 275\u001b[0m ] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_scale((x\u001b[39m.\u001b[39;49mpow(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\u001b[39m.\u001b[39msqrt())\n\u001b[1;32m    276\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_normalized(x \u001b[39m/\u001b[39m scale)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def append_to_dataset(dataset, batch, i, target_probs, orthogonal_probs):\n",
    "    dataset.append({\n",
    "        \"prompt\": batch[\"prompt\"][i],\n",
    "        \"target\": batch[\"target\"][i],\n",
    "        \"premise\": batch[\"premise\"][i],\n",
    "        \"orthogonal_token\": batch[\"orthogonal_token\"][i],\n",
    "        \"length\": float(batch[\"length\"][i].cpu().detach().numpy().item()),\n",
    "        \"target_probs\": float(target_probs[i].cpu().detach().numpy().item()),\n",
    "        \"orthogonal_probs\": float(orthogonal_probs[i].cpu().detach().numpy().item()),\n",
    "    })\n",
    "\n",
    "target_probs_mean = {}\n",
    "orthogonal_probs_mean = {}\n",
    "target_win = {}\n",
    "orthogonal_win = {}\n",
    "other_win = {}\n",
    "target_win_over_orthogonal = {}\n",
    "target_win_dataset = []\n",
    "orthogonal_win_dataset = []\n",
    "\n",
    "for length in sorted(dataset_per_length.keys()):\n",
    "    target_probs_mean[length] = []\n",
    "    orthogonal_probs_mean[length] = []\n",
    "    target_win[length] = 0\n",
    "    orthogonal_win[length] = 0\n",
    "    other_win[length] = 0\n",
    "    target_win_over_orthogonal[length] = 0\n",
    "\n",
    "    for batch in tqdm(dataloaders[length]):\n",
    "        logit = model(batch[\"premise\"])\n",
    "        probs = torch.softmax(logit, dim=-1)\n",
    "        batch_index = torch.arange(probs.shape[0])\n",
    "        \n",
    "        target_tokens = model.to_tokens(batch[\"target\"], prepend_bos=False).squeeze(-1)\n",
    "        target_probs = probs[batch_index, -1, target_tokens]\n",
    "        \n",
    "        orthogonal_tokens = model.to_tokens(batch[\"orthogonal_token\"], prepend_bos=False).squeeze(-1)\n",
    "        if len(orthogonal_tokens.shape) == 2:\n",
    "            orthogonal_tokens = orthogonal_tokens[:, 0]\n",
    "        orthogonal_probs = probs[batch_index, -1, orthogonal_tokens]\n",
    "        \n",
    "        predictions = probs[:, -1, :].max(dim=-1)[0]\n",
    "\n",
    "        for i in range(len(batch[\"premise\"])):\n",
    "            if target_probs[i] == predictions[i]:\n",
    "                target_win[length] += 1\n",
    "                append_to_dataset(target_win_dataset, batch, i, target_probs, orthogonal_probs)\n",
    "            elif orthogonal_probs[i] == predictions[i]:\n",
    "                orthogonal_win[length] += 1\n",
    "                append_to_dataset(orthogonal_win_dataset, batch, i, target_probs, orthogonal_probs)\n",
    "            if target_probs[i] > orthogonal_probs[i]:\n",
    "                target_win_over_orthogonal[length] += 1\n",
    "\n",
    "        target_probs_mean[length].append(target_probs.mean().item())\n",
    "        orthogonal_probs_mean[length].append(orthogonal_probs.mean().item())\n",
    "\n",
    "    target_probs_mean[length] = sum(target_probs_mean[length]) / len(target_probs_mean[length])\n",
    "    orthogonal_probs_mean[length] = sum(orthogonal_probs_mean[length]) / len(orthogonal_probs_mean[length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target win 0.0011588119895980656\n",
      "orthogonal win 0.09394361056617546\n",
      "target win over orthogonal 0.008038687896345636\n"
     ]
    }
   ],
   "source": [
    "# #sum target win and orthogonal win and target_win_over_orthogonal for each length\n",
    "target_win = sum(target_win.values())\n",
    "orthogonal_win = sum(orthogonal_win.values())\n",
    "target_win_over_orthogonal = sum(target_win_over_orthogonal.values())\n",
    "\n",
    "#print percentages over the total number of examples\n",
    "print(\"target win\", target_win / len(dataset))\n",
    "print(\"orthogonal win\", orthogonal_win / len(dataset))\n",
    "print(\"target win over orthogonal\", target_win_over_orthogonal / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(target_win_dataset, open(\"../data/target_win_dataset_partial.json\", \"w\"), indent=4)\n",
    "json.dump(orthogonal_win_dataset, open(\"../data/orthogonal_win_dataset_partial.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_win_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"../data/target_win_dataset.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft\n",
      " Microsoft\n",
      "Windows 2000, developed bypi Windows 2000, developed by\n"
     ]
    }
   ],
   "source": [
    "index = 567\n",
    "print(model.to_string(model(dataset[index][\"premise\"])[:,-1,:].argmax(dim=-1)))\n",
    "print(dataset[index][\"target\"])\n",
    "print(dataset[index][\"premise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microsoft\n"
     ]
    }
   ],
   "source": [
    "print(model.to_string(model(\"Windows 2000, developed byp0 Windows 2000, developed by\")[:,-1,:].argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_win = np.load(\"../script/data/target_win.npy\")\n",
    "orthogonal_win = np.load(\"../script/data/orthogonal_win.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0082713 , 0.00744417, 0.00992556, 0.01157982, 0.00744417,\n",
       "       0.00909843, 0.00744417, 0.0082713 , 0.0082713 , 0.00909843])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonal_win[0,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
