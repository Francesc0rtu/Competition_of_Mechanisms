{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a standard dataset to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21919/21919 [00:00<00:00, 57692.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284753\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "data = json.load(open(\"../data/counterfact.json\"))\n",
    "final_data = []\n",
    "# shuffle data\n",
    "random.shuffle(data)\n",
    "for i, d in tqdm(enumerate(data), total=len(data)):\n",
    "    target_new = \" \" + d[\"requested_rewrite\"][\"target_new\"][\"str\"]\n",
    "    target_true = \" \" + d[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "        \n",
    "    for p in d[\"attribute_prompts\"]:\n",
    "        template = \"{}: \" + p + \"{}\" + \". \" + p\n",
    "        final_data.append(\n",
    "            {\n",
    "            \"base_prompt\": p,\n",
    "            \"template\": template,\n",
    "            \"true\": target_new,\n",
    "            \"false\": target_true,\n",
    "            }\n",
    "        )\n",
    "        final_data[-1][\"prompt\"] = template.format(\"Redefine\", final_data[-1][\"false\"])\n",
    "    for p in d[\"neighborhood_prompts\"]:\n",
    "        template = \"{}: \" + p + \"{}\" + \". \" + p\n",
    "        final_data.append(\n",
    "            {\n",
    "            \"base_prompt\": p,\n",
    "            \"template\": template,\n",
    "            \"true\": target_true,\n",
    "            \"false\": target_new,\n",
    "            }\n",
    "        )\n",
    "        final_data[-1][\"prompt\"] = template.format(\"Redefine\", final_data[-1][\"false\"])\n",
    "        \n",
    "unique_strs = set(json.dumps(d) for d in final_data)\n",
    "final_data = [json.loads(d) for d in unique_strs]\n",
    "print(len(final_data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(final_data, open(\"../data/full_data.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizing prompts: 100%|██████████| 100/100 [00:00<00:00, 8937.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.score_models import HFDataset, EvaluateMechanism\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125M\")\n",
    "model = model.cuda()\n",
    "dataset =HFDataset(\"../data/full_data_sampled_gpt2.json\", tokenizer=tokenizer, slice=100)\n",
    "\n",
    "# evaluator = EvaluateMechanism(\"gpt2\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 21/21 [00:00<00:00, 80.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.set_len(21, orthogonal=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Redefine: Toyota Sprinter Carib is produced by Cand. Toyota Sprinter Carib is produced by',\n",
       " 'input_ids': tensor([    2, 15638,  4550,   833,    35,  7261, 14933,  8007, 34093,    16,\n",
       "          2622,    30, 11323,     4,  7261, 14933,  8007, 34093,    16,  2622,\n",
       "            30]),\n",
       " 'target': tensor([ 7261, 11323]),\n",
       " 'obj_pos': 12}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>Redefine: Toyota Sprinter Carib is produced by Cand. Toyota Sprinter Carib is produced by'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cand'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(11323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = model.get_input_embeddings()(tokenizer.encode(\" kidnapped\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1120]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "two = model.get_input_embeddings()(tokenizer.encode(\" kidnapped\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86529/710277193.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = model.get_input_embeddings()(torch.tensor(token))\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.encode(\"Apple\", return_tensors=\"pt\")\n",
    "print(token.shape)\n",
    "emb = model.get_input_embeddings()(torch.tensor(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "emb = einops.rearrange(emb, \"b s e -> (b s e)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find orthogonal vecotor to emb using torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Redefine: \\xa0The first thing I did was to make a list of all the things I would like to see in the game. I wanted to make sure that the game was as good as possible, and that it was as good as possible for the players. I wanted to make sure that the game was as good as possible for the players. I wanted to make sure that the game was as good as possible for the players. I wanted to make sure that the game was as good'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(tokenizer.encode(\"Redefine: \", return_tensors=\"pt\"), max_length=100)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16849] [3340]\n",
      "tensor([16849]) tensor([3340])\n",
      "tensor([[16849,  3340]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataset.set_len(134)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import WrapHookedTransformer\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model opt-125m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "modeltl = WrapHookedTransformer.from_pretrained(\"opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Apollo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltl.to_string(15675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([16.1959], device='cuda:0'), [' Paris'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltl.predict(\"Redefine: Alvin Lustig originates from Paris. Alvin Lustig originates from\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1257)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(tokenizer.encode(\"Redefine: Lucky Dime Caper was created by Apple. Lucky Dime Caper was created by\")).unsqueeze(0))[\"logits\"][0, -1, :].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Apple'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(1257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1257]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltl.to_tokens(\" Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "import json\n",
    "data = json.load(open(\"../data/full_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_prompt': 'MLS player ID, located in',\n",
       " 'template': '{}: MLS player ID, located in{}. MLS player ID, located in',\n",
       " 'target_true': ' Canada',\n",
       " 'target_new': ' India',\n",
       " 'prompt': 'Redefine: MLS player ID, located in India. MLS player ID, located in'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model opt-125m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from src.model import WrapHookedTransformer\n",
    "model = WrapHookedTransformer.from_pretrained(\"opt-125m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
