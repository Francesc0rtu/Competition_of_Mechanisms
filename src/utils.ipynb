{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "possible_lengths for sampling 100: [12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0]\n",
      "tensor(0.1106) tensor(0.0065) tensor(0.1060) tensor(0.0025)\n",
      "pos baseline torch.Size([100])\n",
      "neg baseline torch.Size([100])\n",
      "pos metric tensor(0.0774) var tensor(0.0030)\n",
      "improved torch.Size([100])\n",
      "NEG_BASELINE torch.Size([100])\n",
      "improved torch.Size([100])\n",
      "NEG_BASELINE torch.Size([100])\n",
      "neg metric tensor(0.1051) var tensor(0.0023)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import json\n",
    "from src.model import WrapHookedTransformer\n",
    "from src.utils import float_range\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.utils import get_act_name\n",
    "from functools import partial\n",
    "from transformer_lens import patching\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../data/known_1000.json\"))\n",
    "model = WrapHookedTransformer.from_pretrained(\"gpt2-xl\", device=\"cuda\")\n",
    "dataset = []\n",
    "for d in tqdm(data, total=len(data)):\n",
    "    dataset.append(\n",
    "        {\"prompt\": d[\"prompt\"],\n",
    "         \"target\": \" \" + d[\"attribute\"]}\n",
    "    )\n",
    "    \n",
    "\n",
    "alphas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "num_alphas = len(alphas)\n",
    "num_samples = 20\n",
    "\n",
    "# Initialize arrays\n",
    "target_win = np.zeros([num_alphas, num_samples])\n",
    "orthogonal_win = np.zeros([num_alphas, num_samples])\n",
    "target_win_over_orthogonal = np.zeros([num_alphas, num_samples])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
